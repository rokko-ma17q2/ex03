---
title: 'ex03: Simple regression'
author: |
  | Kenji Sato
  | Kobe University^[Email: mail@kenjisato.jp]
date: '`r format(Sys.time(), "%B %d, %Y")`'
output:
  bookdown::pdf_document2:
    fig_caption: yes
    includes:
      in_header: asset/latex/custom.sty
    keep_tex: no
    latex_engine: pdflatex
    number_sections: yes
    template: null
    toc: no
  bookdown::html_document2: default
documentclass: scrartcl
---

# Overview

## Purpose {-}

Become familiar with simple linear regression with R.

## Instructions {-}

In this assignment, you will

- clone the assignment repository and make a working branch (eg. `solution` branch);
- solve the problems in Section \@ref(problems); 
- write the solutions in `R/solution.R` and knit `solution.Rmd` the file; 
- commit `solution.Rmd` and `solution.pdf`; and 
- open a Pull Request.

# Simple regression

Although this course does not dig deep into statistics and econometrics, 
you might want to have basic understandings of linear regressions. 

In this exercise, we will use **tidyverse**. Run the following command.

```{r setup, message=FALSE}
library(tidyverse)
```

## Purely linear case

The first data we play with is the following: 

```{r}
df_lin <- 
  tibble(
    x = c(1.37379088091951, 0.862106871728239, 0.473489104395616, 
          0.701262814146907, -0.0850552741258509, 1.56870212341502, 
          0.817391973588289, 0.394768602709471, 1.21269855279069, 
          0.35508065933832), 
    y = c(2.63376468487512, 2.24957802290499, 2.0103939202368, 
          2.21703232314625, 2.02570067955709, 2.4756136111241, 
          2.39195731014444, 1.98947852908374, 2.33804041458729, 
          2.1358293292897))
```

Any analysis bigins with a simple scatter plot.

```{r}
(p_lin <- ggplot(df_lin, aes(x, y)) + geom_point())
```

Do you see a linear relation between `x` and `y`? 
You guessed correctly.


Usually, we do not know the data generating process but in this exercise 
I reveal how I created the data. 

```{r}
set.seed(300)
(x <- rnorm(10))

e <- rnorm(10, 0, 0.1)
(y <- 2 + 0.3 * x + e)
```

The data was generated by the following rule: 
\begin{align}
y = 0.3 x + e,  (\#eq:model)
\end{align}
where the error term, $e$, is drawn from the normal distribution with 
mean = 0 and standard deviation = 0.1.

In a typical data analysis task, we want to reconstruct \@ref(eq:model) from 
given data. More precisely, we start from assuming that the data is generated 
by a linear relationship

$$y = \beta_0 + \beta_1 x$$

and then determine what values of coefficients $(\beta_0, \beta_1)$ best 
explain the data. 

If we could know the coefficient $0.3$, we can interpret predict 
that an increase in $x$ by $1$ would increase $y$ by $0.3$. 

This problem of linear regression is easily solved with R by `lm()` 
function.^[That it is easy does not mean that you can skip serious study of 
econometrics. You must understand, at least in the long run, what is going on 
under the hood.]

```{r ols-1}
(fit_lin <- lm(y ~ x, data = df_lin))
```

Expressions like `y ~ x` are called model formulas. 

* Use `y ~ x` to regress `y` on `x`, 
* Use `z ~ x + y` to regress `z` on `x` and `y`, etc.

R (or its statistical procedure) estimates that the above data is generated by 

$$
y = 1.9629 + 0.36990x
$$

This answer is not particularly appealing but considering the small number 
of data, we can say that linear regression is doing a good job.

You can study the statistical properties in more detail with `summary()` function.

```{r}
summary(fit_lin)
```


## Log linear case

Besides linear case, we often encounter log linear case. Observe the following data.

```{r}
z <- 10 * exp(2 * x + e)
df_loglin <- tibble(x = x, z = z)

ggplot(df_loglin, aes(x, z)) + geom_point()
```

This data doesn't seem to follow a linear law but if taken log, linearity comes out.

```{r}
ggplot(df_loglin, aes(x, log(z))) + geom_point()
```

Mathematically, if the data is generated by 

$$
z = ae^{bx + e},
$$

by taking log of both size we obtain

$$
\ln z = \ln a + b x + e.
$$

These coefficients can easily be estimated with `lm()` function. 

```{r}
lm(log(z) ~ x, data = df_loglin)
```

The coefficients for `x` is estimated at $2.070$. This means that
an increase of $x$ by $1$ will cause an increase of $z$ by $100\times b$ percent.
You can observe this percentage rule with 

$$
b = \frac{d(\ln z)}{dx} = \frac{1}{z} \frac{dz}{dx}
$$
or
$$
\frac{dz}{z} = b \cdot dx
$$


## Other cases

We have seen two cases for which we applied the following formulas.

* `y ~ x`
* `log(y) ~ x`

As you might expect, the following formulas are also valid. Construct 
simulation data yourself and experiment with these specifications.

* `y ~ log(x)`
* `log(y) ~ log(x)`


# Problems

Conduct simple linear regression exercises with the following datasets, 
which are bundled with R:

* `women`
* `cars`
* `pressure`


Interpret each result.

Write your report in `solution.Rmd`, knit it to produce `solution.pdf` 
and commit the two files.
